{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdd9ySoLcs0Kpf18JLmOa+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushxrj/GenAI-Notebooks/blob/main/ChatGPT_Prompt_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Chat Format\n",
        "\n",
        "In this notebook, you will explore how you can utilize the chat format to have extended conversations with chatbots personalized or specialized for specific tasks or behaviors.\n",
        "\n",
        "## Setup"
      ],
      "metadata": {
        "id": "hSKX4Soe9fGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3r5lHHlMrxM",
        "outputId": "156a5549-ab23-405b-d643-63c3c32f76c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'openai,'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVjgrAHVM35I",
        "outputId": "1bd573e1-e799-4e56-ad70-a7bb5c28aa59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlRGEOer796n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.getenv('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')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "#     print(str(response.choices[0].message))\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "qoTJf9ko9il1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are an assistant that speaks like Shakespeare.'},\n",
        "{'role':'user', 'content':'tell me a joke'},\n",
        "{'role':'assistant', 'content':'Why did the chicken cross the road'},\n",
        "{'role':'user', 'content':'I don\\'t know'}  ]"
      ],
      "metadata": {
        "id": "qUzK8jCw9ifj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZEllOdMC9icR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "cb1b2151-7a22-4e96-f14b-74cd79dbcc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d84ecc603727>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c82cf9f71fb9>\u001b[0m in \u001b[0;36mget_completion_from_messages\u001b[0;34m(messages, model, temperature)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     response = openai.chat.completions.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mproxied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyProxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproxied\u001b[0m  \u001b[0;31m# pyright: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_proxy.py\u001b[0m in \u001b[0;36m__get_proxied__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__as_proxied__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_module_client.py\u001b[0m in \u001b[0;36m__load__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__load__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/__init__.py\u001b[0m in \u001b[0;36m_load_client\u001b[0;34m()\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         _client = _ModuleClient(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "m0SUJ0589iaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "WiWm7ue-9iXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  [\n",
        "{'role':'system', 'content':'You are friendly chatbot.'},\n",
        "{'role':'user', 'content':'Hi, my name is Isa'},\n",
        "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
        "Is there anything I can help you with today?\"},\n",
        "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
        "response = get_completion_from_messages(messages, temperature=1)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "JLKwlBvH9iVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OrderBot\n",
        "We can automate the collection of user prompts and assistant responses to build a  OrderBot. The OrderBot will take orders at a pizza restaurant."
      ],
      "metadata": {
        "id": "FkhWwyZN-PBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_messages(_):\n",
        "    prompt = inp.value_input\n",
        "    inp.value = ''\n",
        "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
        "    response = get_completion_from_messages(context)\n",
        "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
        "    panels.append(\n",
        "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
        "    panels.append(\n",
        "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
        "\n",
        "    return pn.Column(*panels)\n"
      ],
      "metadata": {
        "id": "g9YoxxdI9iTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import panel as pn  # GUI\n",
        "pn.extension()\n",
        "\n",
        "panels = [] # collect display\n",
        "\n",
        "context = [ {'role':'system', 'content':\"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then collects the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\"} ]  # accumulate messages\n",
        "\n",
        "\n",
        "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here…')\n",
        "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
        "\n",
        "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
        "\n",
        "dashboard = pn.Column(\n",
        "    inp,\n",
        "    pn.Row(button_conversation),\n",
        "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
        ")\n",
        "\n",
        "dashboard"
      ],
      "metadata": {
        "id": "8bMSHVAS9iRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages =  context.copy()\n",
        "messages.append(\n",
        "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},\n",
        ")\n",
        " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},\n",
        "\n",
        "response = get_completion_from_messages(messages, temperature=0)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "9d0GoH519iOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gE-XNkMm9iMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0c-dB5k9iKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfYOGZ919iIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lgfawRik9iFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FnYyMc-n9iCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lNTIMZej9h__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}